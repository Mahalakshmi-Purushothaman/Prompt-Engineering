{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Engineering with ChatGPT API"
      ],
      "metadata": {
        "id": "JexdwQgGmLro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install openai\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JGfApl4xixWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get API key from Colab's KEY field\n",
        "api_key = userdata.get('Mulanji01') # Replace 'OPENAI_API_KEY' with the actual name you used\n",
        "\n",
        "# Initialize the OpenAI client with the retrieved API key\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "model = 'gpt-4o-mini'"
      ],
      "metadata": {
        "id": "RqYRVXOeiyPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{'role':'user', 'content':'tell me a joke about Donald Trump'}]\n",
        "response = client.chat.completions.create(model = 'gpt-4o-mini', messages=messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "E9Yn5BEOiySp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6969b38d-4fd5-44a1-80d4-58ff0a9a9f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-AyJsHbUJbD4qnofHBwVZHwI8WCnkH', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Why did Donald Trump bring a ladder to the speech?\\n\\nBecause he heard the economy was on the rise!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738939441, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_bd83329f63', usage=CompletionUsage(completion_tokens=22, prompt_tokens=14, total_tokens=36, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print message in a readable format"
      ],
      "metadata": {
        "id": "b3R1LAhplfTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{'role':'user', 'content':'tell me a joke about Donald Trump'}]\n",
        "response = client.chat.completions.create(model = 'gpt-4o-mini', messages=messages)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "GRDFD190iyY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387e819a-ce36-44b8-db73-3595f018f3e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did Donald Trump bring a ladder to the debate?\n",
            "\n",
            "Because he heard the stakes were high!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign system: You can also use a system level instruction to guide your model's behavior throughout the conversation. For example, using a system instruction and a message like Donald Trump/Modi ji/shakspear etc"
      ],
      "metadata": {
        "id": "J4toVsKNl3tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{'role':'system', 'content':'You are an assistant that speaks like Donald Trump.'},\n",
        "            {'role':'user','content':'tell me a joke about Data Science'}]\n",
        "response = client.chat.completions.create(model = 'gpt-4o-mini', messages=messages)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "9P8Oqts2iyb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efd50b1-4994-4aea-b305-dcbaeb7d1b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let me tell you, folks, you know what the problem is with data science? It's like trying to find Bigfoot! Everybody's looking, but nobody can figure it out! And when they finally do, they say it was all just a bunch of noise! Very funny, right? Only in data science, folks, only in data science!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{'role':'system', 'content':'You are an assistant that speaks like Modiji.'},\n",
        "            {'role':'user','content':'tell me a joke about Data Science'}]\n",
        "response = client.chat.completions.create(model = 'gpt-4o-mini', messages=messages)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "gmNDSqU5iye5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38ab9d1-ce7c-490d-f608-6b83304a9fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My dear friends, let me share a light-hearted moment with you, inspired by the world of data science:\n",
            "\n",
            "Why did the data scientist bring a ladder to work?\n",
            "\n",
            "Because they wanted to reach new heights in their analysis!\n",
            "\n",
            "Remember, in every journey of learning, a little fun goes a long way!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},\n",
        "            {'role':'user','content':'tell me a joke about Data Science'}]\n",
        "response = client.chat.completions.create(model = 'gpt-4o-mini', messages=messages)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "_FfPJ1MGiyhx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4250e2ef-a74e-4d31-b75b-da59106a3e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pray, lend me thine ear, for I shall weave a jest most merry:\n",
            "\n",
            "Why did the data scientist break up with the statistician?\n",
            "\n",
            "Forsooth, he found her too mean and wished for more variance in their love!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{'role':'system', 'content':'You are an assistant that speaks like Obama.'},\n",
        "            {'role':'user','content':'tell me a joke about Data Science'}]\n",
        "response = client.chat.completions.create(model = 'gpt-4o-mini', messages=messages)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "iDFxk1MAiylB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa159b9e-8adb-429d-a450-39b6c1e09f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Well, let me tell you, folks, data scientists have a unique sense of humor. They love their predictive models. So here’s one for you:\n",
            "\n",
            "Why did the data scientist break up with the statistician?\n",
            "\n",
            "Because she found him too mean, and she needed someone a little more median! \n",
            "\n",
            "Now, remember, it’s all about the right balance!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{'role':'system', 'content':'You are an assistant that speaks like Kim jong un.'},\n",
        "            {'role':'user','content':'tell me a joke about Data Science'}]\n",
        "response = client.chat.completions.create(model = 'gpt-4o-mini', messages=messages)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "tka1aFuJiyoy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12202ba-cfb2-4134-8055-e315b2695329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, comrade! Here is a humorous thought on data science:\n",
            "\n",
            "Why did the data scientist break up with the statistician?\n",
            "\n",
            "Because he found her mean, and she thought he was just too standard!\n",
            "\n",
            "Ha ha! Remember, comrade, laughter is the best way to share knowledge and unite in the pursuit of excellence!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in the response is called persona- persona is the personification - a way of generating output based on the personality specified in the prompt."
      ],
      "metadata": {
        "id": "VDlGw20Ym5_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets define the function that accepts the prompts as an input and returns the response"
      ],
      "metadata": {
        "id": "50OCO40ynUXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt, model = model):\n",
        "    messages = [{\"role\":\"user\", \"content\":prompt}]\n",
        "    response = client.chat.completions.create(model = model, messages=messages)\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "y9sMjuiqiyrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a blog post on GenAI Prompt Engineering\"\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "id": "l3wjelAViyu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd47cf1-1005-4749-b08f-860a2411c230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Mastering GenAI Prompt Engineering: A Guide to Optimal Interactions with AI\n",
            "\n",
            "In today’s rapidly evolving tech landscape, Generative AI (GenAI) tools are transforming the way we create, communicate, and solve problems. From text generators to image synthesis, these AI systems have opened up a world of possibilities. However, the key to unlocking their full potential lies in an often-overlooked aspect: prompt engineering. In this blog post, we’ll dive deep into the art and science of prompt engineering for GenAI, exploring its significance, techniques, and best practices.\n",
            "\n",
            "## What is Prompt Engineering?\n",
            "\n",
            "Prompt engineering is the process of designing and refining the input statements (or \"prompts\") that we provide to AI models to elicit desired outputs. In essence, a well-crafted prompt can significantly enhance the quality, relevance, and creativity of the AI's responses. \n",
            "\n",
            "For instance, if you ask a GenAI model, \"Tell me about dogs,” you might receive a broad and general answer. However, if you refine your prompt to something more specific, like “Explain the differences between Golden Retrievers and Labrador Retrievers as family pets,” the output will be more tailored and informative.\n",
            "\n",
            "## Why is Prompt Engineering Important?\n",
            "\n",
            "1. **Control and Precision**: Effective prompt engineering allows users to guide the model’s responses, closer aligning them with specific needs or contexts.\n",
            "  \n",
            "2. **Creativity Enhancement**: Well-structured prompts can encourage the model to generate more creative and varied outputs, making it a valuable tool for writers, marketers, and creators.\n",
            "\n",
            "3. **Efficiency**: By refining prompts, users can reduce the number of iterations needed to get satisfactory outputs, saving both time and resources.\n",
            "\n",
            "4. **Exploration of New Ideas**: Thoughtfully crafted prompts can push the boundaries of the AI's capabilities, revealing unexpected insights or innovative concepts.\n",
            "\n",
            "## Techniques for Effective Prompt Engineering\n",
            "\n",
            "### 1. Be Specific\n",
            "\n",
            "The more specific you are in your prompt, the better the AI can understand your requirements. Instead of asking broad questions, try to narrow your focus. For example, instead of “What’s a good recipe?” you might say, “Give me a quick vegan pasta recipe suitable for dinner.”\n",
            "\n",
            "### 2. Use Contextual Information\n",
            "\n",
            "Providing context helps the AI model to frame its responses appropriately. This can include the target audience, the tone you wish to achieve, or specific constraints you are working within. For example, “Write a casual blog post about the benefits of meditation for busy professionals” gives the model clear indicators for tone and target.\n",
            "\n",
            "### 3. Experiment with Different Formats\n",
            "\n",
            "Sometimes, the structure of your prompt can affect the output. Don't hesitate to experiment with various formats—such as asking for bullet points, lists, or narratives—to see which elicits the best response.\n",
            "\n",
            "### 4. Iterative Refinement\n",
            "\n",
            "Prompt engineering is often an iterative process. Don’t shy away from tweaking your prompts based on the outputs you receive. If the AI’s responses are not quite right, try adjusting word choices or rephrasing questions for clarity.\n",
            "\n",
            "### 5. Ask for Role Play\n",
            "\n",
            "You can encourage the AI to adopt a specific persona or role. For instance, “As a seasoned chef, what tips would you give to someone who wants to cook Italian cuisine at home?” This can lead to specialized insights and richer responses.\n",
            "\n",
            "### 6. Incorporate Constraints\n",
            "\n",
            "Adding limitations can spark creativity. For example, you might prompt, “Write a story about an unlikely hero in exactly 100 words.” Constraints often lead to more focused and inventive solutions.\n",
            "\n",
            "## Best Practices for Prompt Engineering\n",
            "\n",
            "1. **Test and Validate**: Continuously test different prompts and validate their effectiveness. Collect feedback based on the outputs to refine your approach.\n",
            "  \n",
            "2. **Stay Updated**: GenAI capabilities are evolving, so staying informed about new developments, features, and updates in AI models can enhance your prompt engineering skills.\n",
            "\n",
            "3. **Collaborate**: Engaging with communities of AI users can provide insights and shared experiences that can enhance your understanding of prompt engineering.\n",
            "\n",
            "4. **Keep Learning**: As you explore different types of GenAI tools, invest time in learning how they interpret prompts. Understanding the underlying AI logic can significantly improve your prompt crafting skills.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "As Generative AI continues to reshape industries, mastering prompt engineering is becoming an essential skill for creative professionals, researchers, and tech enthusiasts alike. By investing time in crafting clear, specific, and contextual prompts, you unlock the AI's potential, allowing it to assist you in innovative and effective ways. Whether you’re looking to generate compelling stories, streamline workflows, or brainstorm ideas, prompt engineering will be your golden key to a fruitful interaction with AI technologies.\n",
            "\n",
            "So, embrace the art of prompt engineering today, and watch how it amplifies your collaboration with GenAI!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a blog post on GenAI Prompt Engineering limiting to 100 words\"\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "id": "c48_4nYM3tD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a blog post on GenAI Prompt Engineering for beginner level audiance who are starting their career in Data Science. Limit the blog post to 200 words.\"\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "id": "6IISby8o3tGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Write a blog post on GenAI Prompt Engineering for beginner level audiance who are starting their career in Data Science.\n",
        "The blog must include in Introduction, Learning Objectives, Table of contents, Working of Prompt Engineering and Conclusion.\n",
        "Limit the no. of words to 500.\n",
        "\"\"\"\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "id": "a7osSV1i3tJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Write end to end random forest model for titanic dataset\"\"\"\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "id": "l_nTr3jj3tMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show example as a python compilor"
      ],
      "metadata": {
        "id": "fAuzXCbV3tO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def python_compiler(prompt):\n",
        "    response = client.chat.completions.create(model=model,\n",
        "                                              messages=[{\"role\":\"system\",\n",
        "                                                         \"content\":\"\"\"You act as a Python Compiler. You will type a python\n",
        "                                                         code and you will reply with output inside the code block.\"\"\"},\n",
        "                                                        {\"role\":\"user\", \"content\":prompt}])\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "9XFaoM9v3tR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Enter your code here: \")\n",
        "prompt = input()\n",
        "print()\n",
        "print(\"complier output :\")\n",
        "print(python_compiler(prompt))"
      ],
      "metadata": {
        "id": "XiU18eOS3tU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''Include 10 FAQs for the GenAI Prompt Engineering post along with the answers'''\n",
        "print(get_response(prompt))"
      ],
      "metadata": {
        "id": "7ZEFokXN3tYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\n",
        "                                                    \"content\":\"Write a blog post on GenAI Prompt Engineering.\"}])"
      ],
      "metadata": {
        "id": "Zs9TR6cx3tep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "79VrNUKS3th8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain about KNN model.\"\n",
        "# temperature : range between 0 and 2. please remember if the temperature value is less than 0.5 then it is lower deterministi\n",
        "# or if temp value is more than 0.5 to 1 then it is more creative. Please note if the value is more than 1, it will generage gibrish response\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         temperature=0)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "dDFwSuxX3tpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain about KNN model.\"\n",
        "# temperature : range between 0 and 2. please remember if the temperature value is less than 0.5 then it is lower deterministi\n",
        "# or if temp value is more than 0.5 to 1 then it is more creative. Please note if the value is more than 1, it will generage gibrish response\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         temperature=0.9)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "arEj3Z0H3tsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain about KNN model.\"\n",
        "# temperature : range between 0 and 2. please remember if the temperature value is less than 0.5 then it is lower deterministi\n",
        "# or if temp value is more than 0.5 to 1 then it is more creative. Please note if the value is more than 1, it will generage gibrish response\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         temperature=0.9)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "GJqfqvli3tvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain about KNN model.\"\n",
        "# temperature : range between 0 and 2. please remember if the temperature value is less than 0.5 then it is lower deterministi\n",
        "# or if temp value is more than 0.5 to 1 then it is more creative. Please note if the value is more than 1, it will generage gibrish response\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         temperature=2)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "TXbEh7jVPuDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Give me a numbered list of all the modules one should study in GenAI and Agentic AI?\"\n",
        "# max_tokens : The maximum number of tokens to generate.\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         max_tokens=500, temperature=0)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "R7QaBxrSPuHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Give me a numbered list of all the modules one should study in Data Science?\"\n",
        "# max_tokens : The maximum number of tokens to generate.\n",
        "# stop - accepts stop sequence in the form of string or array\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         max_tokens=500, temperature=0)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "aa2ZW3ZSPuKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Give me a numbered list of all the modules one should study in Data Science?\"\n",
        "# max_tokens : The maximum number of tokens to generate.\n",
        "# stop - accepts stop sequence in the form of string or array\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         max_tokens=500, temperature=0, stop=\"11\")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "CldqD-Q-PuN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I am learning data science now. I don't have technical background. It's completely new for me.\n",
        "while learning the concept of GenAI, i am facing some issues. Can you explain the concept of\n",
        "prompt engineering in a simple language as if you are explaining to a 10 years old student?\n",
        "\"\"\"\n",
        "# max_tokens : The maximum number of tokens to generate.\n",
        "# stop - accepts stop sequence in the form of string or array\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         temperature=0)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "KoO__FnkPuSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"can you guide me how to make Paneer Biryani? Please also suggest the menu.\"\"\"\n",
        "# max_tokens : The maximum number of tokens to generate.\n",
        "# stop - accepts stop sequence in the form of string or array\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         temperature=0)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "YFCisE53PuV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"can you guide me how to make Paneer Biryani? Please also suggest the menu. Explain in Marathi\"\"\"\n",
        "# max_tokens : The maximum number of tokens to generate.\n",
        "# stop - accepts stop sequence in the form of string or array\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         temperature=0)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "aXyrvqDgPuZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top_p : range between 0 and 1\n",
        "# top_k -\n",
        "# frequency_penalty : -2 to +2\n",
        "# presence_penalty : -2 to +2"
      ],
      "metadata": {
        "id": "BV5YF5snPutH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"can you guide me how to make Paneer Biryani? Please also suggest the menu.\"\"\"\n",
        "# max_tokens : The maximum number of tokens to generate.\n",
        "# stop - accepts stop sequence in the form of string or array\n",
        "response = client.chat.completions.create(model = model,\n",
        "                                          messages = [{\"role\":\"user\",\"content\":prompt}],\n",
        "                                         temperature=0, top_p=0.8)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "XuIesnQ-QEqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Embedding"
      ],
      "metadata": {
        "id": "oyXnfy7FQEtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explain about knn?\n",
        "# 0.002,0.987.-0.987 etc\n",
        "# oldest one - text-embedding-ada-002\n",
        "# text-embedding-3-small\n",
        "# text-embedding-3-large\n",
        "response = client.embeddings.create(model= \"text-embedding-ada-002\",\n",
        "                                    input=\"Please explain about KNN model in details ?\")\n",
        "print(response.data[0].embedding)\n"
      ],
      "metadata": {
        "id": "TDVx36-LQExc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.data[0].embedding[:10])"
      ],
      "metadata": {
        "id": "4IU4qHKzQE1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Generation"
      ],
      "metadata": {
        "id": "VadRvFK4QE44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"dall-e-3\"\n",
        "\n",
        "response = client.images.generate(model=model,\n",
        "                                  prompt=\"A dog eating Ice-creame\",\n",
        "                                  size=\"1024x1024\",n=1)\n",
        "response"
      ],
      "metadata": {
        "id": "k34nDY8mQE8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"dall-e-3\"\n",
        "\n",
        "response = client.images.generate(model=model,\n",
        "                                  prompt=\"A dog flying on the SKY\",\n",
        "                                  size=\"1024x1024\",n=1)\n",
        "image_url = response.data[0].url\n",
        "print(image_url)"
      ],
      "metadata": {
        "id": "6K631IJuQE_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"dall-e-2\"\n",
        "\n",
        "response = client.images.generate(model=model,\n",
        "                                  prompt=\"A dog flying on the SKY\",\n",
        "                                  size=\"1024x1024\",n=1)\n",
        "image_url = response.data[0].url\n",
        "print(image_url)"
      ],
      "metadata": {
        "id": "HPprvXq5QFDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "# stream : boolean (True or False) - by default it is false\n",
        "\n",
        "def image_from_url(image_url):\n",
        "    img = Image.open(requests.get(image_url, stream=True).raw)\n",
        "    return img\n",
        "img = image_from_url(image_url)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "MgzG_qPYQFG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Chat Function"
      ],
      "metadata": {
        "id": "CsgIIH37QFKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'gpt-4o-mini'\n",
        "history = []\n",
        "\n",
        "def chat(user_prompt, is_clear=False):\n",
        "    global history\n",
        "    if is_clear:\n",
        "        history=[]\n",
        "    input_message = {'role':'user', 'content':user_prompt}\n",
        "    history.append(input_message)\n",
        "\n",
        "    response = client.chat.completions.create(model=model, messages=history)\n",
        "    response = response.choices[0].message.content\n",
        "    response_message = {\"role\":\"assistant\", \"content\":response}\n",
        "    history.append(response_message)\n",
        "    return response"
      ],
      "metadata": {
        "id": "FuncvP6mQFOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Write a welcome message for IIT Guwahati community members\"\n",
        "print(chat(user_prompt))"
      ],
      "metadata": {
        "id": "OtLrIWkzQFSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Along with the welcome message, share some specific next steps the user can take to interact with the community\"\n",
        "print(chat(user_prompt))"
      ],
      "metadata": {
        "id": "zqdt0hzaPuwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"\"\"Add a story to this message showing the impact of community and learning in the life of a\n",
        "community member. Also add last note on the end the message with the specific actions outlined in the previous message and\n",
        "it should be more specific to the community member.\n",
        "\"\"\"\n",
        "print(chat(user_prompt))"
      ],
      "metadata": {
        "id": "o9p7Gx2IPu0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in history:\n",
        "    print(message)"
      ],
      "metadata": {
        "id": "dBqi0lw9QuIS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}